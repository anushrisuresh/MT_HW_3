#!/usr/bin/env python

import optparse
import sys
import models
from collections import namedtuple
from functools import cmp_to_key


# precompute costs for traqnslations for future cost estimation
def precompute_costs(f, translation_model, language_model):
    costs = {}
    length = len(f)
    
    def compute_phrase_cost(phrase):
        logprob = phrase.logprob
        language_model_state = tuple()
        for word in phrase.english.split():
            language_model_state, word_logprob = language_model.score(language_model_state, word)
            logprob += word_logprob
        return -logprob
    
    for segment in range(1, length + 1):
        for start_pos in range(length - segment + 1):
            end_pos = start_pos + segment
            costs[(start_pos, end_pos)] = float('inf')

            current_phrase = f[start_pos:end_pos]
            if current_phrase in translation_model:
                costs[(start_pos, end_pos)] = min(compute_phrase_cost(phrase) for phrase in translation_model[current_phrase])

            for phrase_boundary in range(start_pos + 1, end_pos):
                combined_cost = costs.get((start_pos, phrase_boundary), float('inf')) + costs.get((phrase_boundary, end_pos), float('inf'))
                costs[(start_pos, end_pos)] = min(costs[(start_pos, end_pos)], combined_cost)
                
    return costs

# estimates future cost
def estimate_future_cost(coverage, costs):
    future_cost = 0.0
    start = None
    for j, translated in enumerate(coverage):
        if not translated and start is None:
            start = j
        elif translated and start is not None:
            future_cost += costs[(start, j)]
            start = None
    return future_cost

# helper function to compare to hypothesis by log prob
def compare_hypotheses(h1, h2):
    return int(h2.logprob - h1.logprob)

# find indexes for different untranslated segments
def find_untranslated_segments(coverage):
    untranslated_segments = []
    start = -1
    i =0 
    while(i<len(coverage)):
        if not coverage[i]:
            start = i
        else:
            i+=1
            continue
        end = start
        while end < len(coverage):
            if coverage[end]:
                break
            untranslated_segments.append((start,end+1))
            end+=1
        i+=1
    return untranslated_segments

#parent function to decode the sentences
def decode_sentence(f, translation_model, language_model, opts):
    costs = precompute_costs(f, translation_model, language_model)
    initial_coverage = [False for _ in f]
    hypothesis = namedtuple("hypothesis", "logprob, future_cost, language_model_state, predecessor, phrase, coverage")
    initial_hypothesis = hypothesis(0.0, costs[(0, len(f))], language_model.begin(), None, None, initial_coverage)
    beams = [{} for _ in f] + [{}]
    beams[0][language_model.begin()] = initial_hypothesis

    for beam_no, beam in enumerate(beams[:-1]):
        sorted_beam = sorted(beam.values(), key=cmp_to_key(compare_hypotheses))[:opts.s]  # Prune
        for h in sorted_beam:
            untranslated_segments = find_untranslated_segments(h.coverage)

            # for each untranslated segment in h for which there is a translation
            for first, last in untranslated_segments:
                if f[first:last] in translation_model:
                    for phrase in translation_model[f[first:last]]:
                        logprob = h.logprob + phrase.logprob
                        # Building the new coverage vector for our new hypothesis
                        new_coverage = []
                        for k in range(first):
                            new_coverage.append(h.coverage[k])
                        for k in range(first, last):
                            new_coverage.append(True)
                        for k in range(last, len(h.coverage)):
                            new_coverage.append(h.coverage[k])
                        language_model_state = h.language_model_state
                        for word in phrase.english.split():
                            (language_model_state, word_logprob) = language_model.score(language_model_state, word)
                            logprob += word_logprob
                        logprob += language_model.end(language_model_state) if last==len(f) else 0.0
                        # compute new future cost and build new hypothesis
                        new_future_cost = estimate_future_cost(new_coverage, costs)
                        new_hypothesis = hypothesis(logprob, new_future_cost, language_model_state, h, phrase, new_coverage)

                        # recombination
                        s = sum(new_coverage)
                        # partial hypothesis score = logprob - futurecost
                        # if the number of words and the words covered by the hypothesis is different, add to beam
                        # if the number of words and the words covered by the hypothesis is same as an existing hypothesis, 
                        # then prune out the more costly once based on future cost and keep the new hypothesis
                        if language_model_state not in beams[s] or beams[s][language_model_state].logprob - beams[s][language_model_state].future_cost < logprob - new_future_cost:
                            beams[s][language_model_state] = new_hypothesis


    winner = max(beams[-1].values(), key=lambda h: h.logprob)
    return extract_english(winner)

def extract_english(h):
    return "" if h.predecessor is None else "%s%s " % (extract_english(h.predecessor), h.phrase.english)

def main():
    optparser = optparse.OptionParser()
    optparser.add_option("-i", "--input", dest="input", default="data/input", help="File containing French sentences to translate (default=data/input)")
    optparser.add_option("-t", "--translation-model", dest="tm", default="data/tm", help="File containing translation model (default=data/tm)")
    optparser.add_option("-l", "--language-model", dest="lm", default="data/lm", help="File containing ARPA-format language model (default=data/lm)")
    optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxsize, type="int", help="Number of sentences to decode (default=no limit)" )
    optparser.add_option("-k", "--translation-per-phrase", dest="k", default=200, type="int", help="Limit on number of translations to consider per phrase (default=10)")
    optparser.add_option("-s", "--stack-size", dest="s", default=200, type="int", help="Maximum stack size (default=15)")
    optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False, help="Verbose mode (default=off)")

    opts = optparser.parse_args()[0]
    tm = models.TM(opts.tm, opts.k)
    lm = models.LM(opts.lm)
    french = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

    for word in set(sum(french, ())):
        if (word,) not in tm:
            tm[(word,)] = [models.phrase(word, 0.0)]

    sys.stderr.write("Decoding %s...\n" % opts.input)
    for f in french:
        translation = decode_sentence(f, tm, lm, opts)
        print(translation)
        if opts.verbose:
            def extract_translation_model_logprob(h):
                return 0.0 if h.predecessor is None else h.phrase.logprob + extract_translation_model_logprob(h.predecessor)
            translation_model_logprob = extract_translation_model_logprob(translation)
            sys.stderr.write("LM = %f, TM = %f, Total = %f\n" %
            (translation.logprob - translation_model_logprob, translation_model_logprob, translation.logprob))

if __name__ == "__main__":
    main()